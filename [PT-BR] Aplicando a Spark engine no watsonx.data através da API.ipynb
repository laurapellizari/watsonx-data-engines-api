{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando a Spark engine no watsonx.data através da API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração das credenciais\n",
    "\n",
    "Esta célula define as credenciais para acessar o watsonx.data. Na próxima seção mostraremos como capturá -las. As credenciais incluem:\n",
    "\n",
    "wxd_hms_endpoint: o endpoint do serviço watsonx.data\n",
    "\n",
    "wxd_hms_username: o nome de usuário para acessar o watsonx.data\n",
    "\n",
    "wxd_hms_password: a senha para acessar o watsonx.data\n",
    "\n",
    "source_bucket_endpoint: o endpoint do bucket de armazenamento de dados\n",
    "\n",
    "source_bucket_access_key: a chave de acesso para o bucket de armazenamento de dados\n",
    "\n",
    "source_bucket_secret_key: a chave secreta para o bucket de armazenamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wxd_hms_endpoint = \"thrift://aaaaaaaa-bbbb-cccc-dddd-abcdef012345.abcdefghijk0123456789.lakehouse.appdomain.cloud:12345\"\n",
    "wxd_hms_username = \"ibmlhapikey\"\n",
    "wxd_hms_password = \"wxd_hms_password\"\n",
    "source_bucket_endpoint = \"s3.br-sao.cloud-object-storage.appdomain.cloud\"\n",
    "source_bucket_access_key = \"your_source_access_key\"\n",
    "source_bucket_secret_key = \"source_bucket_secret_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando as credenciais\n",
    "\n",
    "Agora vamos realizar a configuração da conexão com o watsonx.data. Para isso, navegue até a aba de Infrastructure Manager, onde você poderá ver todas as conexões entre engines, catálogos e storages. \n",
    "\n",
    "Para configurar a conexão com a engine escolhida, clique na opção correspondente para abrir uma nova janela com detalhes adicionais das credenciais. Essa janela fornecerá as informações necessárias para estabelecer a conexão segura e eficaz com o watsonx.data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wxd-1.png\" alt=\"wxd-1\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez aberta a janela, procure pela aba de \"Visualizar configurações\" para de fato exibir todas as credenciais necessárias para continuar o fluxo de configuração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wxd-2.png\" alt=\"wxd2\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/wxd-3.png\" alt=\"wxd3\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que você tem as configurações em mãos, é hora de adaptá-las às suas credenciais específicas do Watsonx.data e Cloud Object Storage.\n",
    "\n",
    "Com essas informações em mãos, você pode adaptar as configurações para as suas credenciais específicas. Certifique-se de substituir os valores padrão pelas suas credenciais reais.\n",
    "\n",
    "Lembre-se de que é importante manter suas credenciais seguras e não compartilhá-las com ninguém. Além disso, certifique-se de que você está usando as credenciais corretas para o seu watsonx.data e Cloud Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração Spark para wxd\n",
    "conf=spark.sparkContext.getConf()\n",
    "spark.stop()\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf.setAll([(\"spark.sql.catalogImplementation\", \"hive\"), \\\n",
    "    (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"), \\\n",
    "    (\"spark.sql.iceberg.vectorization.enabled\", \"false\"), \\\n",
    "    (\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\"), \\\n",
    "    (\"spark.sql.catalog.lakehouse.type\", \"hive\"), \\\n",
    "    (\"spark.sql.catalog.lakehouse.uri\", wxd_hms_endpoint), \\\n",
    "    (\"spark.hive.metastore.client.auth.mode\", \"PLAIN\"), \\\n",
    "    (\"spark.hive.metastore.client.plain.username\", wxd_hms_username), \\\n",
    "    (\"spark.hive.metastore.client.plain.password\", wxd_hms_password), \\\n",
    "    (\"spark.hive.metastore.use.SSL\", \"true\"), \\\n",
    "    (\"spark.hive.metastore.truststore.type\", \"JKS\"), \\\n",
    "    (\"spark.hive.metastore.truststore.path\", \"file:///opt/ibm/jdk/lib/security/cacerts\"), \\\n",
    "    (\"spark.hive.metastore.truststore.password\", \"changeit\"), \\\n",
    "    (\"spark.hadoop.fs.s3a.bucket.wxd-silver.endpoint\", source_bucket_endpoint), \\\n",
    "    (\"spark.hadoop.fs.s3a.bucket.wxd-silver.access.key\", source_bucket_access_key), \\\n",
    "    (\"spark.hadoop.fs.s3a.bucket.wxd-silver.secret.key\", source_bucket_secret_key), \\\n",
    "])\n",
    "\n",
    "spark=SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>\n",
    "### Listando os databases watsonx.data\n",
    "\n",
    "\n",
    "Uma vez que temos a conexão pronta, podemos listar os bancos de dados existentes no watsonx.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_databases(spark):\n",
    "    spark.sql(\"show databases from lakehouse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_databases(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables from lakehouse.teste_silver\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na célula abaixo, estamos executando uma consulta em Spark para recuperar dados do lakehouse, transformando a consulta para Pandas e medindo o tempo de execução da consulta.\n",
    "\n",
    "Isso nos permite entender como o Spark está lidando com os dados e identificar possíveis otimizações.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_spark = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  *\n",
    "FROM\n",
    "  lakehouse.teste_silver.cadastro\n",
    "\"\"\").toPandas()\n",
    "\n",
    "df_spark_time = time.time() - start_time\n",
    "\n",
    "df_spark.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
